{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BzmCovNKkwbi"
   },
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7TI5hirlzn8"
   },
   "outputs": [],
   "source": [
    "# #@title Install the required libs\n",
    "# %pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
    "# %pip install -q accelerate transformers ftfy fairscale bitsandbytes gradio natsort safetensors xformers datasets pytorch_lightning timm \n",
    "# %pip install -qq \"ipywidgets>=7,<8\"\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guDgmswnmW-4"
   },
   "outputs": [],
   "source": [
    "#@title Import required libraries\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import concurrent\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "#import bitsandbytes as bnb\n",
    "from torch.utils.data import DataLoader\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid_w = cols * w + (cols - 1)  # 计算网格宽度，包括图像间隔\n",
    "    grid_h = rows * h + (rows - 1)  # 计算网格高度，包括图像间隔\n",
    "\n",
    "    grid = Image.new('RGBA', size=(grid_w, grid_h))\n",
    "    transparent_pixel = (0, 0, 0, 0)  # 定义透明像素的颜色\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        x = (w + 1) * (i % cols)  # 计算当前图像的x坐标，包括图像间隔\n",
    "        y = (h + 1) * (i // cols)  # 计算当前图像的y坐标，包括图像间隔\n",
    "\n",
    "        for dx in range(w):\n",
    "            for dy in range(h):\n",
    "                pixel = img.getpixel((dx, dy))\n",
    "                grid.putpixel((x + dx, y + dy), pixel)  # 将图像像素粘贴到网格中\n",
    "\n",
    "        # 添加图像间隔列\n",
    "        if i % cols < cols - 1:\n",
    "            for dy in range(h):\n",
    "                grid.putpixel((x + w, y + dy), transparent_pixel)\n",
    "\n",
    "        # 添加图像间隔行\n",
    "        if i // cols < rows - 1:\n",
    "            for dx in range(w):\n",
    "                grid.putpixel((x + dx, y + h), transparent_pixel)\n",
    "\n",
    "    return grid\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import tomesd\n",
    "import os \n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# Grounding DINO\n",
    "import sys\n",
    "sys.path.append('GroundingDINO')\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tag2Text\n",
    "sys.path.append('Tag2Text')\n",
    "from Tag2Text.models import tag2text\n",
    "from Tag2Text import inference\n",
    "import torchvision.transforms as TS\n",
    "\n",
    "    \n",
    "\n",
    "# cfg\n",
    "config_file = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "tag2text_checkpoint = '../tag2text_swin_14m.pth'  # change the path of the model\n",
    "grounded_checkpoint = '../groundingdino_swint_ogc.pth'  # change the path of the model\n",
    "sam_checkpoint = '../sam_vit_h_4b8939.pth'\n",
    "split = \",\"\n",
    "output_dir = \"outputs\"\n",
    "box_threshold = 0.25\n",
    "text_threshold = 0.2\n",
    "iou_threshold = 0.5\n",
    "\n",
    "\n",
    "# initialize Tag2Text\n",
    "normalize = TS.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "transform = TS.Compose([\n",
    "                TS.Resize((384, 384)),\n",
    "                TS.ToTensor(), normalize\n",
    "            ])\n",
    "\n",
    "# filter out attributes and action categories which are difficult to grounding\n",
    "delete_tag_index = []\n",
    "for i in range(3012, 3429):\n",
    "    delete_tag_index.append(i)\n",
    "\n",
    "specified_tags='None'\n",
    "# load model\n",
    "tag2text_model = tag2text.tag2text_caption(pretrained=tag2text_checkpoint,\n",
    "                                    image_size=384,\n",
    "                                    vit='swin_b',\n",
    "                                    delete_tag_index=delete_tag_index)\n",
    "# threshold for tagging\n",
    "# we reduce the threshold to obtain more tags\n",
    "tag2text_model.threshold = 0.64 \n",
    "tag2text_model.eval()\n",
    "tag2text_model = tag2text_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('fastchat')\n",
    "from fastchat.serve.inference import ChatIO, generate_stream,load_model\n",
    "from fastchat.conversation import get_default_conv_template\n",
    "default = \"Please assess the described scene based on the provided prompt and determine the likelihood of each tag appearing in the scene. Assign a score to each tag according to the following criteria:  If a tag is certain to appear, assign a score of 3. If a tag may appear, assign a score of 2. If a tag is unlikely to appear, assign a score of 1.\"\n",
    "class SimpleChatIO(ChatIO):\n",
    "    def prompt_for_input(self, role,prompt,tags) -> str:\n",
    "        return default+'\\n'+\"prompt:\"+prompt+';tags:'+tags\n",
    "    def prompt_for_output(self, role: str):\n",
    "        print(f\"{role}: \", end=\"\", flush=True)\n",
    "\n",
    "    def stream_output(self, output_stream):\n",
    "        pre = 0\n",
    "        output = ''\n",
    "        for outputs in output_stream:\n",
    "            outputs = outputs.strip().split(\" \")\n",
    "            now = len(outputs) - 1\n",
    "            if now > pre:\n",
    "                output = output+\" \".join(outputs[pre:now])\n",
    "                pre = now\n",
    "        output = output+\" \".join(outputs[pre:])\n",
    "        return output\n",
    "chatio = SimpleChatIO()\n",
    "vicuna_path = \"../vicuna\"\n",
    "\n",
    "\n",
    "vicuna, tokenizer = load_model(\n",
    "        vicuna_path, \"cuda\", 4, None, True, True, False\n",
    "    )\n",
    "\n",
    "conv = get_default_conv_template(vicuna_path)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f4D64FI9pI38"
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7IryKE4wq0SZ"
   },
   "outputs": [],
   "source": [
    "# #@title Creating Dataloader\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ftprompts=['airplane','automobile','bird','deer','dog','cat','frog','horse','ship','truck']  # CIFAR labels\n",
    "# ftprompts = pd.DataFrame({'prompts': ftprompts}) #converting prompts list into a pandas dataframe\n",
    "\n",
    "# class CIFAR10Dataset():\n",
    "#     def __init__(self):\n",
    "#         global ftprompts\n",
    "#         self.ftprompts=ftprompts.iloc[:,0]\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.ftprompts)\n",
    "    \n",
    "#     def __getitem__(self,index):\n",
    "#         return self.ftprompts.iloc[index]\n",
    "\n",
    "# #@markdown Please mention the batch size.\n",
    "# batch_size = 5 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "# dataset = CIFAR10Dataset()\n",
    "# finetune_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train = np.load('./data/coco_train.npy', allow_pickle=True).tolist()\n",
    "coco_val = np.load('./data/coco_val.npy', allow_pickle=True).tolist()\n",
    "coco_test = np.load('./data/coco_test.npy', allow_pickle=True).tolist()\n",
    "human_prompts = np.load('./data/human_prompts.npy', allow_pickle=True).tolist()\n",
    "\n",
    "length = 10000#len(coco_train)\n",
    "ftprompts =  coco_train[:length]\n",
    "\n",
    "ftprompts = pd.DataFrame({'prompts': ftprompts})\n",
    "class MSCOCODataset():\n",
    "    def __init__(self):\n",
    "        global ftprompts\n",
    "        self.ftprompts=ftprompts.iloc[:,0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ftprompts)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.ftprompts.iloc[index]\n",
    "\n",
    "#@markdown Please mention the batch size.\n",
    "batch_size = 8 #@param {type:\"integer\"}\n",
    "dataset = MSCOCODataset()\n",
    "finetune_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BWH9vc1kvhvC"
   },
   "source": [
    "## Loading CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJAguhs1d89L"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "model, preprocess = clip.load('ViT-L/14', device=device)\n",
    "params = torch.load(\"../hpc.pt\")['state_dict']\n",
    "model.load_state_dict(params)\n",
    "\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2Model, AutoTokenizer\n",
    "import torch\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
    "caption = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
    ")\n",
    "caption.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0RPeQGHUzUZp"
   },
   "source": [
    "## Evaluating Aesthetic Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s61Ljr9Sd89M"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sam_score(prompt,tags,length):\n",
    "    inp = chatio.prompt_for_input(conv.roles[0],prompt,tags)\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    generate_stream_func = generate_stream\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    gen_params = {\n",
    "        \"model\": vicuna_path,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"stop\": conv.stop_str,\n",
    "        \"stop_token_ids\": conv.stop_token_ids,\n",
    "        \"echo\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "    output_stream = generate_stream_func(vicuna, tokenizer, gen_params, device)\n",
    "    outputs = chatio.stream_output(output_stream)\n",
    "    sam_score = 0\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i].isdigit():\n",
    "            sam_score += int(outputs[i])\n",
    "    return (sam_score-2*length)/(2*length)\n",
    "def get_tags(image_pil):\n",
    "    raw_image = image_pil.resize(\n",
    "                    (384, 384))\n",
    "    raw_image  = transform(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "    res = inference.inference(raw_image , tag2text_model, specified_tags)\n",
    "\n",
    "\n",
    "    text_prompt=res[0].replace(' |', ',')\n",
    "\n",
    "    length = len(text_prompt.split(','))\n",
    "    return text_prompt,length\n",
    "def get_image_score(image,prompt):    #Evaluating Scores if images\n",
    "    with torch.no_grad():\n",
    "   \n",
    "        text = clip.tokenize([prompt]).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features=text_features.to(torch.float16)\n",
    " \n",
    "        tags,length = get_tags(image)\n",
    "        sam_score = get_sam_score(prompt,tags,length) #sam_score\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = caption.generate(**inputs)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    " \n",
    "        captext = clip.tokenize([generated_text]).to(device)\n",
    "        caption_features = model.encode_text(captext)\n",
    "        caption_features /= caption_features.norm(dim=-1, keepdim=True)\n",
    "        cos_sim = torch.cosine_similarity(text_features, caption_features, dim=1) #cap_score\n",
    "   \n",
    "        reward = float(cos_sim)+float(sam_score)\n",
    "        return reward\n",
    "\n",
    "def get_cap_reward(image,prompt): \n",
    "    with torch.no_grad():\n",
    "   \n",
    "        text = clip.tokenize([prompt]).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features=text_features.to(torch.float16)\n",
    " \n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = caption.generate(**inputs)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    " \n",
    "        captext = clip.tokenize([generated_text]).to(device)\n",
    "        caption_features = model.encode_text(captext)\n",
    "        caption_features /= caption_features.norm(dim=-1, keepdim=True)\n",
    "        cos_sim = torch.cosine_similarity(text_features, caption_features, dim=1) #cap_score\n",
    "   \n",
    "        reward = float(cos_sim)\n",
    "        return reward\n",
    "\n",
    "def get_sam_reward(image,prompt): \n",
    "    with torch.no_grad():\n",
    "        tags,length = get_tags(image)\n",
    "        sam_score = get_sam_score(prompt,tags,length) #sam_score\n",
    "\n",
    "        reward = float(sam_score)\n",
    "        return reward    \n",
    "\n",
    "\n",
    "def get_max_score(prompt_list,image_list,index,epoch=0,ARL=True):  #The get_max_score function will return prompt's image with the highest aesthetic score will be chosen for additional fine-tuning.\n",
    "\n",
    "    if ARL:\n",
    "        cap_score_list = []\n",
    "        sam_score_list = []\n",
    "        for i in range(len(prompt_list)):\n",
    "            cap_score = get_cap_reward(image_list[i],prompt_list[i])\n",
    "            sam_score = get_sam_reward(image_list[i],prompt_list[i])\n",
    "            cap_score_list.append(cap_score)\n",
    "            sam_score_list.append(sam_score)\n",
    "            \n",
    "        cap_rankings = sorted(range(len(cap_score_list)), key=lambda x: cap_score_list[x])\n",
    "        sam_rankings = sorted(range(len(sam_score_list)), key=lambda x: sam_score_list[x])\n",
    "        \n",
    "        total_rankings = [cap + sam for cap, sam in zip(cap_rankings, sam_rankings)]\n",
    "        \n",
    "        ftprompts.loc[index, f'Epoch{epoch} Scores'] = min(total_rankings)\n",
    "        return [min(total_rankings), total_rankings.index(min(total_rankings))]\n",
    "\n",
    "    else:\n",
    "        score_list=[]\n",
    "        for i in range(len(prompt_list)):\n",
    "            score_list.append(get_image_score(image_list[i],prompt_list[i]))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ftprompts.loc[index,f'Epoch{epoch} Scores']=max(score_list)\n",
    "        return [max(score_list),score_list.index(max(score_list))]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak1jArUL0eCi"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jv6WYJos0iT5"
   },
   "outputs": [],
   "source": [
    "#@title Settings for the model\n",
    "\n",
    "#@markdown All settings have been configured to achieve optimal outputorch. Changing them is not advisable.\n",
    "\n",
    "#@markdown Enter value for `resolution`.\n",
    "resolution=256 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown Enter value for `num_images_per_prompt`.\n",
    "num_images_per_prompt=10 #@param {type:\"integer\"} \n",
    "\n",
    "#@markdown Enter value for `epochs`. \n",
    "epochs=2 #@param {type:\"integer\"} |\n",
    "\n",
    "#@markdown Enter value for `seed`.\n",
    "generator = torch.Generator(device=device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7gFbnMaLd89N"
   },
   "outputs": [],
   "source": [
    "# @title Setting Stable Diffusion pipeline\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "def get_pipe(amp=True):\n",
    "    if amp:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
    "    else:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id).to(device)\n",
    "    \n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #@markdown Check the `set_progress_bar_config` option if you would like to hide the progress bar for image generation\n",
    "    set_progress_bar_config= True #@param {type:\"boolean\"}\n",
    "    pipe.set_progress_bar_config(disable=set_progress_bar_config) \n",
    "\n",
    "\n",
    "    scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.scheduler = scheduler\n",
    "    pipe.safety_checker = None\n",
    "    return pipe\n",
    "pipe = get_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run training\n",
    "os.environ['MODEL_NAME'] = model_id\n",
    "os.environ['OUTPUT_DIR'] = f\"./CustomModel/\"\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "topk=length\n",
    "training_steps_per_epoch=topk*10\n",
    "os.environ['CHECKPOINTING_STEPS']=str(training_steps_per_epoch)\n",
    "os.environ['RESOLUTION']=str(resolution)\n",
    "os.environ['LEARNING_RATE']=str(9e-6)\n",
    "\n",
    "# remove old account directory\n",
    "try: \n",
    "    shutil.rmtree('./CustomModel')\n",
    "except:\n",
    "    pass\n",
    "try: \n",
    "    shutil.rmtree('./trainingdataset/imagefolder/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "total = 0\n",
    "for epoch in range(epochs+1):\n",
    "  print(\"Epoch: \",epoch)\n",
    "  epoch=epoch\n",
    "  training_steps=str(training_steps_per_epoch*(epoch+1))\n",
    "  os.environ['TRAINING_STEPS']=training_steps\n",
    "  os.environ['TRAINING_DIR'] = f'./trainingdataset/imagefolder/{epoch}'\n",
    "\n",
    "  training_prompts=[]\n",
    "  ftprompts[f'Epoch{epoch} Scores']=np.nan\n",
    "\n",
    "  for step, prompt_list in enumerate(finetune_dataloader):\n",
    "    tomesd.apply_patch(pipe, ratio=0.5)\n",
    "    image=pipe(prompt_list,num_images_per_prompt=num_images_per_prompt,width=resolution,height=resolution).images\n",
    "    image_list=[]\n",
    "\n",
    "    for i in range(int(len(image)/num_images_per_prompt)):\n",
    "      image_list.append(image[i*num_images_per_prompt:(i+1)*num_images_per_prompt])\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "      step_list=[i for i in range(step*batch_size,(step+1)*batch_size)]\n",
    "      prompts_list = [prompt_list] * len(step_list)\n",
    "      score_index=executor.map(get_max_score,prompts_list,image_list,step_list,[epoch for i in range(len(step_list))])\n",
    "\n",
    "    iterator=0\n",
    "    for max_scores in score_index:\n",
    "      training_prompts.append([max_scores[0],image_list[iterator][max_scores[1]],prompt_list[iterator]])\n",
    "      iterator+=1\n",
    "\n",
    "  training_prompts=[row[1:3] for row in sorted(training_prompts,key=lambda x: (x[0]),reverse=True)[:topk]]\n",
    "  training_prompts=pd.DataFrame(training_prompts)\n",
    "\n",
    "  if not os.path.exists(f\"./trainingdataset/imagefolder/{epoch}/train/\"):\n",
    "    os.makedirs(f\"./trainingdataset/imagefolder/{epoch}/train/\")\n",
    "  if not os.path.exists(f\"./CustomModel/\"):\n",
    "    os.makedirs(f\"./CustomModel/\")\n",
    "  for i in range(len(training_prompts)):\n",
    "    training_prompts.iloc[i,0].save(f'./trainingdataset/imagefolder/{epoch}/train/{i}.png')\n",
    "\n",
    "  training_prompts['file_name']=[f\"{i}.png\" for i in range(len(training_prompts))]\n",
    "  training_prompts.columns = ['0','text','file_name']\n",
    "  training_prompts.drop('0',axis=1,inplace=True)\n",
    "  training_prompts.to_csv(f'./trainingdataset/imagefolder/{epoch}/train/metadata.csv',index=False)\n",
    "\n",
    "\n",
    "  if epoch<epochs:\n",
    "    !accelerate launch --num_processes=1 --mixed_precision='fp16' --dynamo_backend='no' --num_machines=1 train_text_to_image_lora.py \\\n",
    "        --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "        --train_data_dir=$TRAINING_DIR \\\n",
    "        --resolution=$RESOLUTION \\\n",
    "        --train_batch_size=8 \\\n",
    "        --gradient_accumulation_steps=1 \\\n",
    "        --gradient_checkpointing \\\n",
    "        --max_grad_norm=1 \\\n",
    "        --mixed_precision=\"fp16\" \\\n",
    "        --max_train_steps=$TRAINING_STEPS \\\n",
    "        --learning_rate=$LEARNING_RATE \\\n",
    "        --lr_warmup_steps=0 \\\n",
    "        --enable_xformers_memory_efficient_attention \\\n",
    "        --dataloader_num_workers=5 \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --lr_warmup_steps=0 \\\n",
    "        --seed=1234 \\\n",
    "        --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
    "        --resume_from_checkpoint=\"latest\" \\\n",
    "        --lr_scheduler='constant' \n",
    "  pipe.unet.load_attn_procs(f'./CustomModel/')\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pretrained_model_images= True\n",
    "if generate_pretrained_model_images:\n",
    "  image_list=[]\n",
    "  for step, prompt_list in enumerate(finetune_dataloader):\n",
    "      image=pipe(prompt_list,num_images_per_prompt=num_images_per_prompt,width=resolution,height=resolution).images \n",
    "      image_list+=image\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "  grid = image_grid(image_list, len(ftprompts),num_images_per_prompt)\n",
    "  grid.save(\"pretrained.png\") \n",
    "  grid\n",
    "generate_finetuned_model_images= True #@param {type:\"boolean\"}\n",
    "\n",
    "if generate_finetuned_model_images:\n",
    "  image_list=[]\n",
    "  pipe.unet.load_attn_procs('./CustomModel')\n",
    "  for step, prompt_list in enumerate(finetune_dataloader):\n",
    "      image=pipe(prompt_list,num_images_per_prompt=num_images_per_prompt,width=resolution,height=resolution).images \n",
    "      image_list+=image\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "  grid = image_grid(image_list, len(ftprompts),num_images_per_prompt)\n",
    "  grid.save(\"trained.png\")\n",
    "  grid"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BzmCovNKkwbi",
    "f4D64FI9pI38",
    "BWH9vc1kvhvC",
    "0RPeQGHUzUZp",
    "Ak1jArUL0eCi",
    "9U2P_PUN-5xX",
    "rglR5r5gahMv"
   ],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd95ac8400f934ca97b7c7125945f5f2a4616fc88b7668f808354bfbb29c51b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
